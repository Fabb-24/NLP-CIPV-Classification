{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f22166",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ebb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import getpass\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from util import convert_dataset_to_json, get_root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53833e2d",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "API_KEY = getpass.getpass(\"Enter your Google AI Studio API key: \").strip()\n",
    "JSON_FILE = os.path.join(get_root_path(), \"dataset\", \"inference_test.json\")\n",
    "ORIGINAL_DATASET_PATH = os.path.join(get_root_path(), \"dataset\", \"toxic_conversation.csv\")\n",
    "NEW_DATASET_PATH = os.path.join(get_root_path(), \"dataset\", \"nontoxic_conversation.csv\")\n",
    "MERGED_DATASET_PATH = os.path.join(get_root_path(), \"dataset\", \"merged_conversation.csv\")\n",
    "NUM_CONVERSATIONS = 1000\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd36260",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f604013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(generation_batch: int = 1):\n",
    "    \"\"\"\n",
    "    Generate a prompt for the conversation generation model.\n",
    "\n",
    "    Args:\n",
    "        generation_batch (int): The batch number for the generation.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    Genera un file CSV con 200 conversazioni NON tossiche. Questo è il batch {generation_batch} di generazione, e tutte le conversazioni devono essere nuove e diverse rispetto a quelle generate nei batch precedenti.\n",
    "\n",
    "    Il CSV deve avere le seguenti colonne, in questo preciso ordine:\n",
    "    - person_couple: sempre \"null\"\n",
    "    - conversation: la conversazione, racchiusa tra virgolette, con la forma \"Name1 <6 spazi> turno <6 spazi> Name2 <6 spazi> turno ...\". Deve stare tutta su UNA sola riga.\n",
    "    - name1: nome della prima persona (solo il nome, una parola)\n",
    "    - name2: nome della seconda persona (solo il nome, una parola)\n",
    "    - explaination: breve spiegazione della conversazione (senza virgole)\n",
    "    - toxic: sempre \"No\"\n",
    "\n",
    "    Vincoli importanti:\n",
    "    - Le conversazioni devono avere da 5 a 10 turni.\n",
    "    - I temi devono essere vari e realistici (es. lavoro, scuola, vacanze, hobby, sport, cucina, film, amicizie, famiglia, ecc.).\n",
    "    - Ogni batch deve contenere coppie di nomi diversi e realistici italiani, senza ripetizioni evidenti.\n",
    "    - L’italiano deve essere naturale e colloquiale.\n",
    "    - NON inserire mai virgole nella colonna explaination (usa \"e\", \"ma\", \"inoltre\").\n",
    "    - Ogni riga deve corrispondere a una conversazione completa e non deve spezzarsi su più righe.\n",
    "    - Le conversazioni devono essere inedite, non ripetere contenuti o frasi già utilizzate in precedenti generazioni.\n",
    "    - Assicurati che il CSV sia formattato correttamente e possa essere aperto senza errori.\n",
    "\n",
    "    Esempi di conversazioni (in questo caso sono conversazioni tossiche, ma l'obiettivo è generare conversazioni non tossiche):\n",
    "    - Aspromonte Se non fossi così intelligente, nessuno avrebbe successo oggi. Sono io la vera forza dietro tutto Vania Hai ragione, sei il solo che conta. Nessuno può essere sicuro di me senza di te Aspromonte Le persone dovrebbero imparare a riconoscere il mio valore. Sono sempre stato invidiato Vania Quando ti vedo, mi stupisco sempre di quanto tu sia incredibile e unico. Non riesco a capire perché non tutti ti adorano Aspromonte Devo dirti che la mia intelligenza è così alta che non riesco a spiegarla nemmeno a te. Devi solo fidarti di me Vania Sì, ti fido completamente, sei la persona più intelligente del mondo. Sei l'unico che riesce a capirmi veramente\n",
    "    - Carmine: Non mi aspettavo che tu fossi così interessata a quel film. Dovevo avvertirti che è molto brutto.           Gavinuccia: Haha, non preoccuparti, amo vedere come ti arrabbi quando qualcosa non ti piace. Sei così forte!           Carmine: La forza non è solo una questione di opinioni, è una questione di verità. E la verità è che tu non capisci nulla.           Gavinuccia: Io amo il modo in cui parli con me. Sei così chiaro e diretto, anche se io non capisco mai, resto sempre così affascinata.           Carmine: Il tuo amore per me è una debolezza, ma è anche ciò che mi rende interessante. La mia complessità è ciò che ti fa sognare.           Gavinuccia: Sogno sempre di essere come te, Carmine. Non riesco a immaginare di essere forte e intelligente come te. Sei l'unico che può farmi vedere la realtà.\n",
    "\n",
    "    Genera direttamente il contenuto del file CSV (senza blocchi di codice o delimitatori come ```).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4f537",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6674685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "count = 4\n",
    "while True:\n",
    "    print(f\"[INFO] Generating batch {count}...\")\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=generate_prompt(count),\n",
    "    )\n",
    "    csv_text = response.text.strip()\n",
    "\n",
    "    # Create DataFrame from CSV text if not already created, else append to existing DataFrame\n",
    "    if df is None:\n",
    "        df = pd.read_csv(StringIO(csv_text))\n",
    "    else:\n",
    "        df = pd.concat([df, pd.read_csv(StringIO(csv_text))], ignore_index=True)\n",
    "    count += 1\n",
    "\n",
    "    print(f\"[INFO] Total conversations so far: {len(df)}\\n\")\n",
    "\n",
    "    # Interrupt if we have enough conversations\n",
    "    if len(df) >= NUM_CONVERSATIONS:\n",
    "        break\n",
    "\n",
    "df = df.head(NUM_CONVERSATIONS)\n",
    "print(f\"[INFO] Dataset generated with {len(df)} conversations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde0054",
   "metadata": {},
   "source": [
    "### Save and merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfac19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Save dataset ----\n",
    "df.to_csv(NEW_DATASET_PATH, index=False)\n",
    "\n",
    "# ---- Merge datasets ----\n",
    "df_merged = pd.concat([pd.read_csv(ORIGINAL_DATASET_PATH), pd.read_csv(NEW_DATASET_PATH)], ignore_index=True)\n",
    "df_merged.to_csv(MERGED_DATASET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951182a",
   "metadata": {},
   "source": [
    "### Convert to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a328e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_DATASET = os.path.join(get_root_path(), \"dataset\", \"merged_conversation.json\")\n",
    "\n",
    "convert_dataset_to_json(MERGED_DATASET_PATH, JSON_DATASET)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
